
import streamlit as st
import pandas as pd
import io
import base64
import pickle
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseUpload
from datetime import date
import re 


def carregar():
    # Fun√ß√£o para carregar o arquivo
    token_b64 = st.secrets["gmail_token"]["token_b64"]

    pasta_raiz_id = st.secrets["drive_pasta_notas"]["drive_notas"]

    with open("token_gmail.pkl", "wb") as token_file:
        token_file.write(base64.b64decode(token_b64))

    with open("token_gmail.pkl", "rb") as token:
        creds = pickle.load(token)

    drive_service = build("drive", "v3", credentials=creds)

    SCOPES = [
        "https://www.googleapis.com/auth/gmail.send",
        "https://www.googleapis.com/auth/drive.file", 
        "https://www.googleapis.com/auth/drive.metadata",  
    ]
    
    def encontrar_ou_criar_pasta(nome, id_pasta_mae):
        """
        Verifica se a pasta j√° existe dentro da pasta m√£e. Se n√£o existir, cria.
        Retorna o ID da pasta.
        """
        query = f"'{id_pasta_mae}' in parents and name = '{nome}' and mimeType = 'application/vnd.google-apps.folder' and trashed = false"
        resultados = drive_service.files().list(q=query, fields="files(id, name)").execute()
        arquivos = resultados.get("files", [])

        if arquivos:
            return arquivos[0]["id"]
        else:
            # Cria a pasta
            metadata = {
                "name": nome,
                "mimeType": "application/vnd.google-apps.folder",
                "parents": [id_pasta_mae],
            }
            pasta = drive_service.files().create(body=metadata, fields="id").execute()
            return pasta["id"]        
    
    @st.cache_data
    def carregar_dados(arquivo):
        try:
            df = pd.read_excel(arquivo)
            return df
        except Exception as e:
            st.error(f"Erro ao carregar o arquivo: {e}")
            return pd.DataFrame()

    # Fun√ß√£o para limpar os dados
    @st.cache_data
    def limpar_dados(df, prova, etapa, codetapa, codprova, tipoetapa):
        df_aluno = st.session_state["dados"].get("alunosxdisciplinas")
        df_base = df_aluno.copy()

        df_base['RA'] = df_base['RA'].astype(str).str.zfill(7)
        df['RA'] = df['RA'].astype(str).str.zfill(7)
        
        # Renomear colunas para padronizar
        df_base.rename(columns={'NOMEDISCIPLINA': 'DISCIPLINA',
                                'NOMECURSO': 'CURSO',
                                'NOMEALUNO': 'ALUNO'}, inplace=True)
        
        df.rename(columns={'NOMEDISCIPLINA': 'DISCIPLINA',
                        'NOMECURSO': 'CURSO',
                        'NOMEALUNO': 'ALUNO'}, inplace=True)

        # --- üîç Detectar a disciplina e filtrar df_base ---
        if 'DISCIPLINA' in df.columns and not df['DISCIPLINA'].empty:
            disciplina_arquivo = df['DISCIPLINA'].iloc[0]
            df_base = df_base[df_base['DISCIPLINA'] == disciplina_arquivo]
        else:
            st.warning("Coluna 'DISCIPLINA' n√£o encontrada ou vazia no arquivo enviado.")
            return pd.DataFrame()
        
        # --- ‚úÖ Garantir que s√≥ usa colunas existentes no df ---
        colunas_validas = [c for c in ['DISCIPLINA', 'RA', 'NOTAS'] if c in df.columns]
        if len(colunas_validas) < 3:
            st.warning(f"Colunas insuficientes para merge: {colunas_validas}")
            return pd.DataFrame()

        # --- üîó Faz o merge agora de forma segura ---
        df = pd.merge(df_base, df[colunas_validas], on=['DISCIPLINA', 'RA'], how='left')
        
        df = df.copy()
        
        # Adicionar colunas complementares
        df['CODETAPA'] = codetapa
        df['CODPROVA'] = codprova
        df['TIPOETAPA'] = tipoetapa
        df['PROVA'] = prova
        df['ETAPA'] = etapa
        df['RA novo'] = df['RA'].astype(int)
        
        # Reorganizar colunas
        colunas = ['CODCOLIGADA', 'CURSO', 'TURMADISC', 'IDTURMADISC', 'DISCIPLINA',
                'RA', 'ALUNO', 'ETAPA', 'PROVA', 'TIPOETAPA', 'CODETAPA',
                'CODPROVA', 'NOTAS']
        df = df[colunas]

        # Limpeza condicional das notas
        df_teste = df
        if prova == "Prova":
            df_teste = df_teste.dropna(subset=['NOTAS']).copy()
        elif prova in ["Recupera√ß√£o", "Recupera√ß√£o Final"]:
            df_teste = df_teste.dropna(subset=['NOTAS'])
        elif prova == "Quizz":
            df_teste = df_teste.dropna(subset=['NOTAS'])

        return df_teste

    
    def detectar_etapas_provas(df: pd.DataFrame):
        """
        Detecta automaticamente colunas que representam provas (P1, P2, P3)
        e tipos (Prova, Recupera√ß√£o, Quiz). Remove colunas vazias de Quiz.
        Retorna o DataFrame limpo e um dicion√°rio com metadados das colunas.
        """

        mapa_etapa = {"P1": 1, "P2": 2, "P3": 3}
        mapa_prova = {"PROVA": 1, "REC": 2, "QUIZZ": 3}

        # Aceita colunas como "P1", "Quiz P1", "Rec P2", "Recupera√ß√£o P3", "Prova P1"
        padrao = re.compile(r"(?:(QUIZ|QUIZZ|RECUPERA√á√ÉO|REC|PROVA)\s*)?(P1|P2|P3)", re.IGNORECASE)

        mapeamento = {}

        for col in df.columns:
            match = padrao.search(col)
            if match:
                tipo_raw = match.group(1).upper() if match.group(1) else "PROVA"
                etapa = match.group(2).upper()

                # Normaliza "REC" ‚Üí "RECUPERA√á√ÉO"
                if tipo_raw in ["Rec"]:
                    tipo = "RECUPERA√á√ÉO"
                if tipo_raw in ["QUIZ", "QUIZZ"]:
                    tipo = "QUIZZ"
                else:
                    tipo = tipo_raw

                mapeamento[col] = {
                    "etapa": etapa,
                    "prova": tipo,
                    "codetapa": mapa_etapa.get(etapa),
                    "codprova": mapa_prova.get(tipo, 1)
                }

        # === Remover colunas de QUIZ totalmente nulas ===
        colunas_quiz_vazias = [
            c for c in df.columns
            if re.search(r"QUIZ|QUIZZ", c, re.IGNORECASE) and df[c].isna().all()
        ]
        if colunas_quiz_vazias:
            df = df.drop(columns=colunas_quiz_vazias)
            st.info(f"Removidas colunas de Quiz sem dados: {', '.join(colunas_quiz_vazias)}")

        return df, mapeamento



    # Interface do Streamlit
    st.title("Conversor de Notas Totvs")

    # Upload do arquivo Excel
    uploaded_file = st.file_uploader("Envie o arquivo de notas (Excel)", type=["xlsx"])

    # Definir as vari√°veis de configura√ß√£o para o filtro
    tipoetapa = 'N'  # Tipo de etapa
    

    if uploaded_file:
        # 1) Carrega e exibe os dados originais
        df_original = carregar_dados(uploaded_file)
        st.subheader("Dados Originais")
        df_original = df_original.dropna(axis=1, how='all') 
        df, mapeamento= detectar_etapas_provas(df_original)
        st.write(df, mapeamento)
        
        # lista para armazenar resultados por prova+disciplina
        dfs_limpos = []
        
        tipos_provas = set()
        # Loop sobre as colunas detectadas (ex: "P1", "Quiz P1", ...)
        for col, info in mapeamento.items():
            etapa = info["etapa"]
            prova_tipo = info["prova"].capitalize()   # 'Prova', 'Quiz', 'Recupera√ß√£o'
            codetapa = info["codetapa"]
            codprova = info["codprova"]
            
            tipos_provas.add(prova_tipo)
            # renomeia a coluna detectada para NOTAS temporariamente
            temp = df_original.rename(columns={col: "NOTAS"})[["DISCIPLINA", "RA", "NOTAS"]].copy()

            # pega as disciplinas que existem nesse arquivo/coluna (normalmente 1)
            disciplinas_no_arquivo = temp["DISCIPLINA"].dropna().unique().tolist()
            if not disciplinas_no_arquivo:
                st.warning(f"Coluna {col}: n√£o foi encontrada DISCIPLINA v√°lida ‚Äî pulando.")
                continue

            # itera por cada disciplina encontrada (caso o arquivo tenha mais de uma)
            for disciplina in disciplinas_no_arquivo:
                # filtra apenas as linhas da disciplina atual
                df_temp = temp[temp["DISCIPLINA"] == disciplina].copy()

                # se n√£o houver notas v√°lidas nessa coluna para a disciplina, pula
                if df_temp["NOTAS"].dropna().empty:
                    st.info(f"{prova_tipo} {etapa} ‚Äî {disciplina}: sem notas v√°lidas ‚Äî pulando.")
                    continue

                # chama a fun√ß√£o de limpeza (usa s√≥ DISCIPLINA, RA, NOTAS)
                df_limpo = limpar_dados(df_temp, prova_tipo, etapa, codetapa, codprova, tipoetapa)

                if df_limpo is None or df_limpo.empty:
                    st.info(f"{prova_tipo} {etapa} ‚Äî {disciplina}: nenhum registro ap√≥s limpeza.")
                    continue

                dfs_limpos.append({
                    "disciplina": disciplina,
                    "prova": prova_tipo,
                    "etapa": etapa,
                    "codetapa": codetapa,
                    "codprova": codprova,
                    "df": df_limpo
                })

                st.success(f"{prova_tipo} {etapa} ‚Äî {disciplina}: processada ({len(df_limpo)} registros).")

        # concatena todos os dataframes processados (se houver)
        if not dfs_limpos:
            st.warning("Nenhuma coluna de prova v√°lida encontrada ap√≥s filtro por disciplina.")
            st.stop()

        df_final = pd.concat([item["df"] for item in dfs_limpos], ignore_index=True)
        st.subheader("Dados combinados de todas as provas (ap√≥s filtro por disciplina)")
        st.dataframe(df_final)

        df_limpo['RA']    = df_limpo['RA'].astype(str).str.zfill(7)
        df_limpo['NOTAS']= pd.to_numeric(df_limpo['NOTAS'], errors='coerce')
        if (df_limpo['NOTAS'] > 8).any():
            st.info("Existem alunos com nota maior que 8.")

        # 3) Gera e oferece o download do TXT
        output = io.BytesIO()
        df_limpo['NOTAS'] = df_limpo['NOTAS']\
            .apply(lambda x: f"{x:.2f}".replace('.', ',') if isinstance(x, (int, float)) else x)
        df_limpo.to_csv(output, index=False, sep=';', header=False, encoding='utf-8')
        output.seek(0)
        
        # Extrai disciplina, turma e curso do DataFrame original
        disciplina = df_limpo['DISCIPLINA'].iloc[0]
        turma = df_limpo['TURMADISC'].iloc[0]
        curso = df_limpo['CURSO'].iloc[0] 
        
    
        tipos_provas = "_".join(sorted(tipos_provas))
        name_file = f"{disciplina}_{turma}_{tipos_provas}_{etapa}.txt"
        
        clicou = st.download_button(
            label="Baixar Notas Tratadas (TXT)",
            data=output,
            file_name=name_file,
            mime="text/plain"
        )

        excel_buffer = io.BytesIO()
        with pd.ExcelWriter(excel_buffer, engine='xlsxwriter') as writer:
            df_limpo.to_excel(writer, index=False, sheet_name="Notas")
        excel_buffer.seek(0)
        justo = False
        if justo:
            pasta_curso_id      = encontrar_ou_criar_pasta(curso, pasta_raiz_id)
            pasta_turma_id      = encontrar_ou_criar_pasta(turma, pasta_curso_id)
            pasta_disciplina_id = encontrar_ou_criar_pasta(disciplina, pasta_turma_id)
            pasta_prova_id      = encontrar_ou_criar_pasta(prova_tipo, pasta_disciplina_id)

            now = date.today().strftime('%Y-%m-%d')
            nome_xlsx = f"{disciplina}_{turma}_{prova_tipo}_{etapa}_{now}.xlsx"

            media = MediaIoBaseUpload(
                excel_buffer,
                mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
            metadata = {"name": nome_xlsx, "parents": [pasta_prova_id]}

            drive_service.files().create(
                body=metadata,
                media_body=media,
                fields="id,name"
            ).execute()

            st.success(f"Arquivo Excel salvo no Drive: {nome_xlsx}")
